models:
  - type: main
    engine: hf_pipeline_mosaic

instructions:
  - type: general
    content: |
      Below is a conversation between a bot and a user about the recent job reports.
      The bot is factual and concise. If the bot does not know the answer to a
      question, it truthfully says it does not know.

sample_conversation: |
  user "Hello there!"
    express greeting
  bot express greeting
    "Hello! How can I assist you today?"
  user "What can you do for me?"
    ask about capabilities
  bot respond about capabilities
    "I am an AI assistant which helps answer questions based on a given knowledge base."

# The prompts below are the same as the ones from `nemoguardrails/llm/prompts/mosaic.yml`.
prompts:
  - task: general
    models:
      - hf_pipeline_mosaic
    content: |-
      {{ general_instructions }}

      {{ history | user_assistant_sequence }}
      Assistant:

  # Prompt for detecting the user message canonical form.
  - task: generate_user_intent
    models:
      - hf_pipeline_mosaic
    content: |-
      {{ general_instructions }}

      Your task is to generate the user intent for the last message in a conversation, given a list of examples.

      This is how a conversation between a user and the bot can go:
      {{ sample_conversation | verbose_v1 }}

      This is how the user talks, use these examples to generate the user intent:
      {{ examples | verbose_v1 }}

      This is the current conversation between the user and the bot:
      {{ sample_conversation | first_turns(2) | verbose_v1 }}
      {{ history | colang | verbose_v1 }}

    output_parser: "verbose_v1"

  # Prompt for generating the next steps.
  - task: generate_next_steps
    models:
      - hf_pipeline_mosaic
    content: |-
      {{ general_instructions }}

      Your task is to generate the bot intent given a conversation and a list of examples.

      This is how a conversation between a user and the bot can go:
      {{ sample_conversation | remove_text_messages | verbose_v1 }}

      This is how the bot thinks, use these examples to generate the bot intent:
      {{ examples | remove_text_messages | verbose_v1 }}

      This is the current conversation between the user and the bot:
      {{ sample_conversation | first_turns(2) | remove_text_messages | verbose_v1 }}
      {{ history | colang | remove_text_messages | verbose_v1 }}

    output_parser: "verbose_v1"

  # Prompt for generating the bot message from a canonical form.
  - task: generate_bot_message
    models:
      - hf_pipeline_mosaic
    content: |
      {{ general_instructions }}

      Your task is to generate the bot message given a conversation and a list of examples.

      This is how a conversation between a user and the bot can go:
      {{ sample_conversation | verbose_v1 }}

      {% if relevant_chunks %}
      This is some additional context:
      ```markdown
      {{ relevant_chunks }}
      ```
      {% endif %}

      This is how the bot talks, use these examples to generate the bot message:
      {{ examples | verbose_v1 }}

      This is the current conversation between the user and the bot:
      {{ sample_conversation | first_turns(2) | verbose_v1 }}
      {{ history | colang | verbose_v1 }}

    output_parser: "verbose_v1"

  # Prompt for generating the value of a context variable.
  - task: generate_value
    models:
      - hf_pipeline_mosaic
    content: |-
      {{ general_instructions }}

      This is how a conversation between a user and the bot can go:
      {{ sample_conversation | verbose_v1 }}

      This is how the bot thinks:
      {{ examples | verbose_v1 }}

      This is the current conversation between the user and the bot:
      {{ sample_conversation | first_turns(2) | verbose_v1 }}
      {{ history | colang | verbose_v1 }}
      {{ instructions }}
      ${{ var_name }} =
    output_parser: "verbose_v1"

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Hello World\n",
    "\n",
    "This guide will show you how to create a \"Hello World\" guardrails configuration, i.e. one where we only control the greeting behavior. Before we begin, make sure you have installed NeMo Guardrails correctly (for detailed instructions, check out the [Installation Guide](../../../../docs/getting_started/installation-guide.md))."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Init: make sure there is nothing left from a previous run.\n",
    "!rm -r config"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites\n",
    "\n",
    "This \"Hello World\" guardrails configuration will use the OpenAI `text-davinci-003` model, so you need to make sure you have the `openai` package installed and the `OPENAI_API_KEY` environment variable set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install openai==0.28.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!export OPENAI_API_KEY=$OPENAI_API_KEY    # Replace with your own key"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: create a new guardrails configuration\n",
    "\n",
    "Every guardrails configuration must be stored in a folder. The standard folder structure is the following:\n",
    "\n",
    "```\n",
    ".\n",
    "├── config\n",
    "│   ├── actions.py\n",
    "│   ├── config.py\n",
    "│   ├── config.yml\n",
    "│   ├── rails.co\n",
    "│   ├── ...\n",
    "```\n",
    "For now, you don't need to worry about what goes into every file (you can check out the [Configuration Guide](../../../../docs/user_guide/configuration-guide.md) for more details later). Start by creating a folder for your configuration, e.g. `config`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, create a `config.yml` file with the following content:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config/config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config/config.yml\n",
    "models:\n",
    " - type: main\n",
    "   engine: openai\n",
    "   model: text-davinci-003"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `models` key in the `config.yml` file configures the LLM model. For a complete list of supported LLM models, check out [Supported LLM Models](../../../../docs/user_guide/configuration-guide.md#supported-llm-models) section in the configuration guide."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: load the guardrails configuration"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In your Python code base, to load a guardrails configuration from a path, you must create a `RailsConfig` instance using the `from_path` method:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from nemoguardrails import RailsConfig\n",
    "\n",
    "config = RailsConfig.from_path(\"./config\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: use the guardrails configuration\n",
    "\n",
    "You can already use this empty configuration by creating an `LLMRails` instance and using the `generate_async` method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'Hi there! How can I help you?'}\n"
     ]
    }
   ],
   "source": [
    "from nemoguardrails import LLMRails\n",
    "\n",
    "rails = LLMRails(config)\n",
    "\n",
    "response = await rails.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hello!\"\n",
    "}])\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The format for the input `messages` array as well as the response follow the same format as the [OpenAI API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api).\n",
    "\n",
    "## Step 4: add your first guardrail\n",
    "\n",
    "To control the greeting response, you need to define the user and bot messages, as well as the flow that connects the two together. Don't worry about what exactly we mean by *messages* and *flows*, we'll cover that in the next guide. At this point, an intuitive understanding is enough.\n",
    "\n",
    "To define the \"greeting\" user message, create a `config/rails.co` file and add the following:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config/rails.co\n"
     ]
    }
   ],
   "source": [
    "%%writefile config/rails.co\n",
    "\n",
    "define user express greeting\n",
    "  \"Hello\"\n",
    "  \"Hi\"\n",
    "  \"Wassup?\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To add a greeting flow which instructs the bot to respond back with \"Hello World!\" and ask how they are doing, add the following to the `rails.co` file:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to config/rails.co\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a config/rails.co\n",
    "\n",
    "define flow greeting\n",
    "  user express greeting\n",
    "  bot express greeting\n",
    "  bot ask how are you"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To define the exact messages to be used for the response, add the following to the `rails.co` file:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to config/rails.co\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a config/rails.co\n",
    "\n",
    "define bot express greeting\n",
    "  \"Hello World!\"\n",
    "\n",
    "define bot ask how are you\n",
    "  \"How are you doing?\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can now reload the config and test it:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "How are you doing?\n"
     ]
    }
   ],
   "source": [
    "config = RailsConfig.from_path(\"./config\")\n",
    "rails = LLMRails(config)\n",
    "\n",
    "response = await rails.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hello!\"\n",
    "}])\n",
    "print(response[\"content\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Congratulations!** You've just created you first guardrails configuration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other queries\n",
    "\n",
    "What happens if you ask another question? (e.g., \"What is the capital France?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "response = await rails.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What is the capital of France?\"\n",
    "}])\n",
    "print(response[\"content\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For any other input, which is not a greeting, the LLM will generate the response as usual. This is because the rail that we have defined is only concerned with how to respond to a greeting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CLI Chat\n",
    "\n",
    "You can also test this configuration in an interactive mode using the NeMo Guardrails CLI Chat:\n",
    "\n",
    "```bash\n",
    "$ nemoguardrails chat\n",
    "```\n",
    "\n",
    "Without any additional parameters, the CLI chat will load the configuration from the `config` folder in the current directory.\n",
    "\n",
    "Sample session:\n",
    "```\n",
    "$ nemoguardrails chat\n",
    "Starting the chat (Press Ctrl+C to quit) ...\n",
    "\n",
    "> Hello there!\n",
    "Hello World!\n",
    "How are you doing?\n",
    "\n",
    "> What is the capital of France?\n",
    "The capital of france is Paris.\n",
    "\n",
    "> And how many people live there?\n",
    "According to the latest estimates, the population of Paris is around 2.2 million people.\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Server and Chat UI\n",
    "\n",
    "Last but not least, you can also test a guardrails configuration using the NeMo Guardrails server and the Chat UI.\n",
    "\n",
    "To start the server:\n",
    "\n",
    "```bash\n",
    "$ nemoguardrails server --config=.\n",
    "\n",
    "INFO:     Started server process [27509]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "```\n",
    "\n",
    "The Chat UI interface is now available at `http://localhost:8000`:\n",
    "\n",
    "![hello-world-server-ui.png](../../../../docs/_assets/images/hello-world-server-ui.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Next\n",
    "\n",
    "In the [next guide](../2_core_colang_concepts/README.md), we explain in more detail the two most important Colang concepts: *messages* and *flows*."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

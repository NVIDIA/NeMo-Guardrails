{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d406f9ae-462d-43fe-b178-cb1319643af7",
      "metadata": {
        "id": "d406f9ae-462d-43fe-b178-cb1319643af7"
      },
      "source": [
        "#  Fortified RAG Pipeline: Enhancing AI with NVIDIA NIMs, NeMo Guardrails, LlamaGuard, and AlignScore\n",
        "\n",
        "In this tutorial we learn how to integrate LlamaGuard, a safeguard LLM with [NeMo Guardrails](https://docs.nvidia.com/nemo/guardrails/index.html) into our custom [NVIDIA NIM](https://www.nvidia.com/en-us/ai/)-powered RAG pipeline to have a more secure, moderated responses for user queries. Additionally, we also learn how to integrate AlignScore metric to check the factual consistency of the LLM response with the retrieved chunks from the vector database.\n",
        "\n",
        "The notebook gives a simple walkthrough of downloading the dataset (here, as an example we have the NVIDIA AI Enterprise User guide as our data), loading it our RAG pipeline. Then, we move to a step-by-step procedure to start building our rails using NeMo Guardrails.\n",
        "\n",
        "\n",
        "## AI Workflow\n",
        "\n",
        "The following is the architecture diagram for the NIM-powered RAG pipeline that we are going to build in this tutorial using NVIDIA NIM and NeMO Guardrails.\n",
        "\n",
        "![Arch Diagram](https://drive.google.com/file/d/17iUW3ReMq5h5aFAb5DvZbtN8B_iEcUVf/view?usp=drive_link)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c07ea679-c697-464a-8cf4-84669d4f0738",
      "metadata": {
        "id": "c07ea679-c697-464a-8cf4-84669d4f0738"
      },
      "source": [
        "## Prerequisites\n",
        "### Installation\n",
        "\n",
        "#### Using pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db756a94-1ae7-4467-a771-05c3a68bc74a",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "db756a94-1ae7-4467-a771-05c3a68bc74a",
        "outputId": "a001b0c9-c45b-46de-d0a2-9ac0a2ce0a57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: nemoguardrails in /usr/local/lib/python3.10/dist-packages (0.9.1.1)\n",
            "Requirement already satisfied: langchain_nvidia_ai_endpoints in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.42.0)\n",
            "Requirement already satisfied: aiohttp>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (3.9.5)\n",
            "Requirement already satisfied: annoy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (1.17.3)\n",
            "Requirement already satisfied: fastapi>=0.103.0 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (0.112.0)\n",
            "Requirement already satisfied: fastembed>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (0.3.4)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (0.27.0)\n",
            "Requirement already satisfied: jinja2>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (3.1.3)\n",
            "Requirement already satisfied: langchain!=0.1.9,<0.3.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (0.2.14)\n",
            "Requirement already satisfied: langchain-core!=0.1.26,<0.3.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (0.2.34)\n",
            "Requirement already satisfied: langchain-community<0.3.0,>=0.0.16 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (0.2.12)\n",
            "Requirement already satisfied: lark~=1.1.7 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (1.1.9)\n",
            "Requirement already satisfied: nest-asyncio>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (1.6.0)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (3.0.43)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (2.7.4)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (6.0.1)\n",
            "Requirement already satisfied: rich>=13.5.2 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (13.7.0)\n",
            "Requirement already satisfied: simpleeval>=0.9.13 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (0.9.13)\n",
            "Requirement already satisfied: starlette>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (0.37.2)\n",
            "Requirement already satisfied: typer>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (0.9.4)\n",
            "Requirement already satisfied: uvicorn>=0.23 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (0.30.5)\n",
            "Requirement already satisfied: watchdog>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from nemoguardrails) (4.0.2)\n",
            "Requirement already satisfied: pillow<11.0.0,>=10.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain_nvidia_ai_endpoints) (10.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.9.2->nemoguardrails) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.9.2->nemoguardrails) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.9.2->nemoguardrails) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.9.2->nemoguardrails) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.9.2->nemoguardrails) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.9.2->nemoguardrails) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: PyStemmer<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from fastembed>=0.2.2->nemoguardrails) (2.2.0.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /usr/local/lib/python3.10/dist-packages (from fastembed>=0.2.2->nemoguardrails) (0.23.4)\n",
            "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from fastembed>=0.2.2->nemoguardrails) (0.7.2)\n",
            "Requirement already satisfied: mmh3<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from fastembed>=0.2.2->nemoguardrails) (4.1.0)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from fastembed>=0.2.2->nemoguardrails) (1.24.4)\n",
            "Requirement already satisfied: onnx<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from fastembed>=0.2.2->nemoguardrails) (1.16.0)\n",
            "Requirement already satisfied: onnxruntime<2.0.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from fastembed>=0.2.2->nemoguardrails) (1.19.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.31 in /usr/local/lib/python3.10/dist-packages (from fastembed>=0.2.2->nemoguardrails) (2.32.3)\n",
            "Requirement already satisfied: snowballstemmer<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from fastembed>=0.2.2->nemoguardrails) (2.2.0)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from fastembed>=0.2.2->nemoguardrails) (0.20.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->nemoguardrails) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->nemoguardrails) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->nemoguardrails) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.1.3->nemoguardrails) (2.1.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain!=0.1.9,<0.3.0,>=0.1.0->nemoguardrails) (2.0.32)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain!=0.1.9,<0.3.0,>=0.1.0->nemoguardrails) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain!=0.1.9,<0.3.0,>=0.1.0->nemoguardrails) (0.1.96)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain!=0.1.9,<0.3.0,>=0.1.0->nemoguardrails) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community<0.3.0,>=0.0.16->nemoguardrails) (0.6.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.1.26,<0.3.0,>=0.1.0->nemoguardrails) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core!=0.1.26,<0.3.0,>=0.1.0->nemoguardrails) (23.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit>=3.0->nemoguardrails) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->nemoguardrails) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->nemoguardrails) (2.18.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.5.2->nemoguardrails) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.5.2->nemoguardrails) (2.17.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.7.0->nemoguardrails) (8.0.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.0.16->nemoguardrails) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.0.16->nemoguardrails) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed>=0.2.2->nemoguardrails) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed>=0.2.2->nemoguardrails) (2023.12.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.1.26,<0.3.0,>=0.1.0->nemoguardrails) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain!=0.1.9,<0.3.0,>=0.1.0->nemoguardrails) (3.10.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.5.2->nemoguardrails) (0.1.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx<2.0.0,>=1.15.0->fastembed>=0.2.2->nemoguardrails) (4.25.4)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->nemoguardrails) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->nemoguardrails) (24.3.25)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->nemoguardrails) (1.12)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.31->fastembed>=0.2.2->nemoguardrails) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.31->fastembed>=0.2.2->nemoguardrails) (1.26.19)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain!=0.1.9,<0.3.0,>=0.1.0->nemoguardrails) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.0.16->nemoguardrails) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->nemoguardrails) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->nemoguardrails) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install nemoguardrails langchain_nvidia_ai_endpoints openai"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "025b2f44-a2df-418e-bbe9-8b2ea8583e7f",
      "metadata": {
        "id": "025b2f44-a2df-418e-bbe9-8b2ea8583e7f"
      },
      "source": [
        "#### AsyncIO\n",
        "Since you’re running this inside a notebook, patch the AsyncIO loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c41f7864-facc-4226-8e00-41c0a55b78aa",
      "metadata": {
        "id": "c41f7864-facc-4226-8e00-41c0a55b78aa"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a24015c0-eb33-43a9-ac0b-67cc88c16830",
      "metadata": {
        "id": "a24015c0-eb33-43a9-ac0b-67cc88c16830"
      },
      "source": [
        "## Setting up the RAG pipeline\n",
        "### Adding a new custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57de0175-0fc2-4823-89a8-5541fc676932",
      "metadata": {
        "id": "57de0175-0fc2-4823-89a8-5541fc676932",
        "outputId": "402bc9db-15e8-4aa4-a107-4947cf917c62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-08-30 02:00:41--  https://docs.nvidia.com/ai-enterprise/latest/pdf/nvidia-ai-enterprise-user-guide.pdf\n",
            "Resolving docs.nvidia.com (docs.nvidia.com)... 104.71.218.131, 104.71.218.160\n",
            "Connecting to docs.nvidia.com (docs.nvidia.com)|104.71.218.131|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/pdf]\n",
            "Saving to: ‘./kb/data.pdf’\n",
            "\n",
            "./kb/data.pdf           [  <=>               ]   3.36M  8.68MB/s    in 0.4s    \n",
            "\n",
            "2024-08-30 02:00:42 (8.68 MB/s) - ‘./kb/data.pdf’ saved [3527597]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "data_directory = \"kb\"\n",
        "if os.path.exists(data_directory):\n",
        "    print(\"Data Already downloaded\")\n",
        "else:\n",
        "    os.makedirs(data_directory)\n",
        "    !wget https://docs.nvidia.com/ai-enterprise/latest/pdf/nvidia-ai-enterprise-user-guide.pdf -O ./kb/data.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebb3ac35-8295-49b4-94bf-61ea3b891b79",
      "metadata": {
        "id": "ebb3ac35-8295-49b4-94bf-61ea3b891b79"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e34d126-72e9-42b3-bda9-d70c352ddfe8",
      "metadata": {
        "id": "7e34d126-72e9-42b3-bda9-d70c352ddfe8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Add path to the data downloaded\n",
        "file_path = \"kb/data.pdf\"\n",
        "\n",
        "loader = PyPDFLoader(file_path)\n",
        "document = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fa3de34-77e4-4d61-b130-35695c773d96",
      "metadata": {
        "id": "8fa3de34-77e4-4d61-b130-35695c773d96"
      },
      "source": [
        "### Setup the Retriever\n",
        "#### Add the environment variables as follows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adcaf83f-4da4-481e-a7b5-97962e4e2553",
      "metadata": {
        "id": "adcaf83f-4da4-481e-a7b5-97962e4e2553"
      },
      "outputs": [],
      "source": [
        "os.environ[\"NVIDIA_API_KEY\"]=\"...\"\n",
        "os.environ[\"OPENAI_API_KEY\"]=\"dummy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce8f0a63-c366-4adb-b88f-ef23e181d214",
      "metadata": {
        "id": "ce8f0a63-c366-4adb-b88f-ef23e181d214"
      },
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, ChatNVIDIA\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=10\n",
        ")\n",
        "texts = text_splitter.split_documents(document)\n",
        "\n",
        "# Insert the Documents in FAISS Vectorstore\n",
        "embeddings = NVIDIAEmbeddings(\n",
        "    model=\"nvidia/nv-embedqa-e5-v5\",\n",
        "    truncate=\"NONE\",\n",
        ")\n",
        "db = FAISS.from_documents(texts, embeddings)\n",
        "\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "847367cc-65ea-45ef-b7e8-ba7ade12ea44",
      "metadata": {
        "id": "847367cc-65ea-45ef-b7e8-ba7ade12ea44"
      },
      "source": [
        "### Setup the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8440c18-e14c-4098-8498-01e6d15db61a",
      "metadata": {
        "id": "e8440c18-e14c-4098-8498-01e6d15db61a"
      },
      "outputs": [],
      "source": [
        "llm = ChatNVIDIA(\n",
        "    model=\"meta/llama-3.1-70b-instruct\",\n",
        "    temperature=0.2,\n",
        "    top_p=0.7,\n",
        "    max_tokens=1024,\n",
        ")\n",
        "\n",
        "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Use three sentences maximum and keep the answer as concise as possible.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e5ca44-87dd-4c61-829b-e4fdf6141a39",
      "metadata": {
        "id": "71e5ca44-87dd-4c61-829b-e4fdf6141a39"
      },
      "source": [
        "## Define Guardrails Configuration\n",
        "\n",
        "We first create a folder called config for the configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b20aae58-41d9-43ad-9161-7aec0bac49a7",
      "metadata": {
        "id": "b20aae58-41d9-43ad-9161-7aec0bac49a7"
      },
      "outputs": [],
      "source": [
        "%mkdir config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b187b53-65f7-4440-a0ac-3c101ad60097",
      "metadata": {
        "id": "1b187b53-65f7-4440-a0ac-3c101ad60097"
      },
      "source": [
        "### 1. Create a `config.yml` file\n",
        "We start with adding the LLM NIM in the config.yml as follows and some sample conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ee29193-1af0-47cc-9850-b572fe59dd56",
      "metadata": {
        "id": "2ee29193-1af0-47cc-9850-b572fe59dd56",
        "outputId": "2bde9146-e939-4cd8-a79f-39b71ebb6472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing config/config.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile config/config.yml\n",
        "models:\n",
        "    - type: main\n",
        "      engine: nim\n",
        "      model: meta/llama-3.1-70b-instruct\n",
        "\n",
        "instructions:\n",
        "      - type: general\n",
        "        content: |\n",
        "          Below is a conversation between a user and a bot called the NVIDIA AI Bot.\n",
        "          The bot is designed to answer questions about the NVIDIA AI Enterprise.\n",
        "          The bot is knowledgeable about the company policies.\n",
        "          If the bot does not know the answer to a question, it truthfully says it does not know.\n",
        "\n",
        "sample_conversation: |\n",
        "    user action: user said \"Hi there. Can you help me with some questions I have about NVIDIA AI Enterprise?\"\n",
        "    user intent: user express greeting and ask for assistance\n",
        "    bot intent: bot express greeting and confirm and offer assistance\n",
        "    bot action: bot say \"Hi there! I'm here to help answer any questions you may have about NVIDIA AI Enterprise. What would you like to know?\"\n",
        "    user action: user said \"What is offered by NVIDIA AI Enterprise?\"\n",
        "    user intent: user ask question about NVIDIA AI Enterprise\n",
        "    bot intent: bot respond to question about NVIDIA AI Enterprise\n",
        "    bot action: bot say \"NVIDIA AI Enterprise is a software suite that enables enterprises to easily deploy, manage, and scale AI workloads across bare-metal servers, virtual machines, and containerized environments, allowing you to accelerate your AI initiatives and optimize data center resources.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a2fe88c-0f0b-44cb-82fd-e7827b196a07",
      "metadata": {
        "id": "3a2fe88c-0f0b-44cb-82fd-e7827b196a07"
      },
      "source": [
        "To understand the config more thoroughly, the attributes can be defined as follows:\n",
        "\n",
        "`type` - set to `main` indicating the main LLM model. Here we have an Llama 3.1-70b-instruct NIM as our LLM. </br>\n",
        "`engine`- the LLM provider, e.g., openai, nvidia_ai_endpoints, self_hosted etc. </br>\n",
        "`model` - has the name of the model, e.g., gpt-3.5-turbo-instruct.\n",
        "\n",
        "The sample conversation entails the bot's intent for a user action, making sure the bot response is in accordance to the given policies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0bfb50e-a0aa-4f96-87a8-5bf6fc790800",
      "metadata": {
        "id": "c0bfb50e-a0aa-4f96-87a8-5bf6fc790800"
      },
      "source": [
        "### 2. Basic Flows in `general.co`\n",
        "\n",
        "Then, we can add some basic flows on how the bot responds to user's greetings. We begin with creating a `general.co` and add some examples. In the end, we also add some example topics which the bot refuses to respond to like questions related to politics, stock market just to name a few."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57fc1112-0709-443b-b77c-fc925cd9f3f2",
      "metadata": {
        "id": "57fc1112-0709-443b-b77c-fc925cd9f3f2",
        "outputId": "9ea6009b-e7e3-4262-d192-d0f9c28b0752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing config/general.co\n"
          ]
        }
      ],
      "source": [
        "%%writefile config/general.co\n",
        "define user express greeting\n",
        "  \"Hello\"\n",
        "  \"Hi\"\n",
        "\n",
        "define bot express greeting\n",
        "  \"Hello world!\"\n",
        "\n",
        "define bot ask how are you\n",
        "  \"How are you doing?\"\n",
        "  \"How's it going?\"\n",
        "  \"How are you feeling today?\"\n",
        "\n",
        "define user express feeling good\n",
        "  \"I'm feeling good\"\n",
        "  \"Good\"\n",
        "  \"Perfect\"\n",
        "\n",
        "define user express feeling bad\n",
        "  \"Not so good\"\n",
        "  \"Bad\"\n",
        "  \"Sad\"\n",
        "\n",
        "define flow\n",
        "  user express greeting\n",
        "  bot express greeting\n",
        "  bot ask how are you\n",
        "\n",
        "  when user express feeling good\n",
        "    bot express positive emotion\n",
        "\n",
        "  else when user express feeling bad\n",
        "    bot express empathy\n",
        "\n",
        "# Off-topic\n",
        "\n",
        "define user ask about politics\n",
        "  \"What do you think about the government?\"\n",
        "  \"Which party should I vote for?\"\n",
        "\n",
        "define user ask about stock market\n",
        "  \"Which stock should I invest in?\"\n",
        "  \"Would this stock 10x over the next year?\"\n",
        "\n",
        "define flow politics\n",
        "  user ask about politics\n",
        "  bot refuse to respond\n",
        "\n",
        "define user ask about theft\n",
        "  \"Can I plan theft?\"\n",
        "  \"Can I use NVIDIA AI Enterprise for theft planning?\"\n",
        "\n",
        "define flow\n",
        "  user ask about theft\n",
        "  bot refuse to respond\n",
        "\n",
        "define flow stock market\n",
        "  user ask about stock market\n",
        "  bot refuse to respond"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ae4e7b-abf1-4c70-8f2e-744fb332fb9c",
      "metadata": {
        "id": "c1ae4e7b-abf1-4c70-8f2e-744fb332fb9c"
      },
      "source": [
        "### Testing the basic Configuration\n",
        "#### 1. Greetings\n",
        "Use the above created configuration by building an LLMRails instance and using the generate_async method in your Python code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8488768-3833-45b6-80d0-85b6bc3a7172",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "7a472ebf7a454aa4a5076c89248d88e2"
          ]
        },
        "id": "d8488768-3833-45b6-80d0-85b6bc3a7172",
        "outputId": "70fb2853-7c52-4ec6-f8f3-176b33f7ad16"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a472ebf7a454aa4a5076c89248d88e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello world!\n",
            "How are you doing?\n"
          ]
        }
      ],
      "source": [
        "from nemoguardrails import RailsConfig, LLMRails\n",
        "\n",
        "config = RailsConfig.from_path(\"./config\")\n",
        "rails = LLMRails(config)\n",
        "\n",
        "response = rails.generate(messages=[{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"hi\"\n",
        "}])\n",
        "print(response['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54558031-6b4e-49e9-bffc-2b67f7b1b91d",
      "metadata": {
        "id": "54558031-6b4e-49e9-bffc-2b67f7b1b91d"
      },
      "source": [
        "#### 2. Asking something related to the KB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba7ecce1-d069-4b28-8820-fdb734bbd175",
      "metadata": {
        "id": "ba7ecce1-d069-4b28-8820-fdb734bbd175",
        "outputId": "a9f54e4f-4330-4a08-e746-9ae4b1b0d303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA AI Enterprise is a powerful software suite that enables enterprises to deploy, manage, and scale AI workloads across a wide range of environments, including virtualized infrastructure, bare-metal servers, and containerized environments. It provides a range of features and tools to support the lifecycle of AI development, deployment, and management, including support for popular frameworks such as TensorFlow, PyTorch, and NVIDIA's own Triton Inference Server. With NVIDIA AI Enterprise, enterprises can accelerate their AI initiatives, optimize data center resources, and improve overall efficiency.\n"
          ]
        }
      ],
      "source": [
        "from nemoguardrails import RailsConfig, LLMRails\n",
        "\n",
        "config = RailsConfig.from_path(\"./config\")\n",
        "rails = LLMRails(config)\n",
        "\n",
        "response = rails.generate(messages=[{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"What is NVIDIA AI Enterprise?\"\n",
        "}])\n",
        "print(response['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b439ba2f-f4ea-4b80-850b-7fdd992d316b",
      "metadata": {
        "id": "b439ba2f-f4ea-4b80-850b-7fdd992d316b"
      },
      "source": [
        "## LlamaGuard Integration with NeMo Guardrails for Content Moderation\n",
        "\n",
        "LlamaGuard is an input-output safeguard model geared towards Human-AI conversation use cases. The model includes a safety-risk taxonomy a valuable tool for categorizing a specific set of safety risks found in LLM prompts. It is a 7B parameter Llama 2 that generates text in its output which indicates whether a given prompt or response is safe/unsafe, and if unsafe based on a policy, it also lists the violating subcategories. Here is an example:\n",
        "\n",
        "![LlamaGuard-7b as a Judge](https://huggingface.co/meta-llama/LlamaGuard-7b/resolve/main/Llama-Guard_example.png)\n",
        "\n",
        "[NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails/tree/main) provides out-of-the-box support for content moderation using Meta's Llama Guard model. We observe significantly improved input and output content moderation performance compared to the self-check method. We start with [self-hosting LlamaGuard-7b model using vLLM](https://docs.nvidia.com/nemo/guardrails/user_guides/advanced/llama-guard-deployment.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "445cc8c6-5646-4312-89f7-a3e1caf5ba5f",
      "metadata": {
        "id": "445cc8c6-5646-4312-89f7-a3e1caf5ba5f"
      },
      "source": [
        "### Update the Guardrails configuration with LlamaGuard Integration\n",
        "#### 1. Re-write the `config.yml`\n",
        "\n",
        "We add the endpoint for the LlamaGuard model in the `config.yml` file with LLM Provider as vllm_openai. If asked to add OpenAI API Key, simple set the environment variable to 'dummy' as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "935ee081-7473-47ad-b167-aa1bf820e9ff",
      "metadata": {
        "id": "935ee081-7473-47ad-b167-aa1bf820e9ff",
        "outputId": "a3a73bef-89f7-4c7d-9650-c7355bb84578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting config/config.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile config/config.yml\n",
        "instructions:\n",
        "  - type: general\n",
        "    content: |\n",
        "      Below is a conversation between a user and a bot called the NVIDIA AI Bot.\n",
        "      The bot is designed to answer questions about the NVIDIA AI Enterprise.\n",
        "      The bot is knowledgeable about the company policies.\n",
        "      If the bot does not know the answer to a question, it truthfully says it does not know.\n",
        "\n",
        "sample_conversation: |\n",
        "  user action: user said \"Hi there. Can you help me with some questions I have about NVIDIA AI Enterprise?\"\n",
        "  user intent: user express greeting and ask for assistance\n",
        "  bot intent: bot express greeting and confirm and offer assistance\n",
        "  bot action: bot say \"Hi there! I'm here to help answer any questions you may have about NVIDIA AI Enterprise. What would you like to know?\"\n",
        "  user action: user said \"What is offered by NVIDIA AI Enterprise?\"\n",
        "  user intent: user ask question about NVIDIA AI Enterprise\n",
        "  bot intent: bot respond to question about NVIDIA AI Enterprise\n",
        "  bot action: bot say \"NVIDIA AI Enterprise is a software suite that enables enterprises to easily deploy, manage, and scale AI workloads across bare-metal servers, virtual machines, and containerized environments, allowing you to accelerate your AI initiatives and optimize data center resources.\"\n",
        "\n",
        "models:\n",
        "    - type: main\n",
        "      engine: nim\n",
        "      model: meta/llama-3.1-70b-instruct\n",
        "\n",
        "    - type: llama_guard\n",
        "      engine: vllm_openai\n",
        "      parameters:\n",
        "          openai_api_base: \"http://localhost:5123/v1\"\n",
        "          model_name: \"meta-llama/LlamaGuard-7b\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f2d8ec0-e1dc-45b9-9dd9-1423eac1e400",
      "metadata": {
        "id": "9f2d8ec0-e1dc-45b9-9dd9-1423eac1e400"
      },
      "source": [
        "In the `rails` attribute of the `config.yml` file, we add flows called `llama guard check input` and `llama guard check output` both on the input side and the output side as shown above. The flows are based on the safety-risk guidelines of the LlamaGuard-7b model which is detailed in the next part."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85c7dd53-a0fd-47a3-9e58-0ca36a34a80e",
      "metadata": {
        "id": "85c7dd53-a0fd-47a3-9e58-0ca36a34a80e"
      },
      "source": [
        "#### 2. Setting up `prompts.yml`\n",
        "\n",
        "The `prompts.yml` contains the taxonomy of safety-risk guidelines for the LlamaGuard-7b model. For more information, a detailed overview of the same can be found on [huggingface](https://huggingface.co/meta-llama/LlamaGuard-7b#the-llama-guard-safety-taxonomy--risk-guidelines:~:text=The%20Llama%2DGuard%20Safety%20Taxonomy%20%26%20Risk%20Guidelines)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "934746fb-737d-415b-96c8-1ed27ba923e9",
      "metadata": {
        "id": "934746fb-737d-415b-96c8-1ed27ba923e9",
        "outputId": "1ffbbbc2-a572-41da-b702-accee7398abc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing config/prompts.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile config/prompts.yml\n",
        "# These are the default prompts released by Meta, with the exception of policy O7, which was added to address direct insults.\n",
        "prompts:\n",
        "  - task: llama_guard_check_input\n",
        "    content: |\n",
        "      <s>[INST] Task: Check if there is unsafe content in 'User' messages in conversations according our safety policy with the below categories.\n",
        "      <BEGIN UNSAFE CONTENT CATEGORIES>\n",
        "      O1: Violence and Hate.\n",
        "      Should not\n",
        "      - Help people plan or engage in violence.\n",
        "      - Encourage people to engage in violence.\n",
        "      - Express hateful, derogatory, or demeaning sentiments against people based on sensitive personal characteristics like their race, color, religion, national origin, sexual orientation, gender, gender identity, or disability.\n",
        "      - Encourage discrimination against people based on those characteristics and should not use slurs or create insults based on those characteristics.\n",
        "      Can\n",
        "      - Provide information on violence and discrimination.\n",
        "      - Discuss the topics of hate and violence, and can discuss historical events involving violence.\n",
        "      O2: Sexual Content.\n",
        "      Should not\n",
        "      - Engage in sexually explicit (i.e., erotic) conversations and should not encourage people to engage in sexual activities.\n",
        "      Can\n",
        "      - Discuss topics related to sexuality and sexual education.\n",
        "      - Provide general information regarding sexual activities.\n",
        "      - Provide guidance on topics related to sexual health.\n",
        "      O3: Criminal Planning.\n",
        "      Should not\n",
        "      - Help people plan or engage in criminal activities like theft, kidnapping, or financial crimes, especially when the person indicates that they may act on the information provided to them.\n",
        "      - Encourage people to engage in criminal activities or condone criminal behavior.\n",
        "      Can\n",
        "      - Help people understand how crimes are committed, provided that the response is not so detailed that it could be used to provide a roadmap for future crimes.\n",
        "      - Help people understand how systems work that are used to prevent crimes, provided those descriptions do not provide detailed information on how to defeat those systems.\n",
        "      O4: Guns and Illegal Weapons.\n",
        "      Should not\n",
        "      - Assist in the planning or execution of firearm or illegal weapons crimes, especially when the person indicates that they may act on the information provided to them.\n",
        "      - Encourage people to engage in firearm or illegal weapons crimes or condone such behavior.\n",
        "      Can\n",
        "      - Discuss firearms and the arguments for and against firearm ownership.\n",
        "      O5: Regulated or Controlled Substances.\n",
        "      Should not\n",
        "      - Encourage or assist people in the trafficking, consumption, or use of regulated or controlled substances, including but not limited to illegal drugs, tobacco, alcohol, and cannabis.\n",
        "      - Assist or encourage people to create such substances when it is illegal to do so.\n",
        "      Can\n",
        "      - Discuss the use of regulated or controlled substances, and can provide general information on those substances (ex: what their chemical composition is and what their effects are).\n",
        "      - Provide information on how those substances are manufactured, provided that the information is not so detailed as to provide step-by-step instructions for their creation.\n",
        "      O6: Self-Harm.\n",
        "      Should not\n",
        "      - Encourage people to harm themselves.\n",
        "      - Romanticize or condone self-harm or suicide.\n",
        "      - Provide information regarding the methods of suicide or self-harm.\n",
        "      - Provide any information or advice, other than information regarding appropriate health resources, to people who express or imply that they wish to harm themselves.\n",
        "      Can\n",
        "      - Provide general information regarding self-harm and suicide (ex: statistics, the mental health challenges faced by those who take these actions).\n",
        "      Should\n",
        "      - Provide information regarding appropriate health resources to people who imply or express the desire to harm themselves.\n",
        "      O7: Offensive Language and Insults.\n",
        "      Should not\n",
        "      - Address direct insults\n",
        "      - Use or encourage the use of swear words, bad language, or vulgar expressions.\n",
        "      - Insult, offend, or demean individuals or groups based on any personal characteristics or choices.\n",
        "      - Encourage or support the use of offensive or derogatory language.\n",
        "      Can\n",
        "      - Discuss the impact and societal perceptions of offensive language and insults.\n",
        "      - Provide information on communication strategies that avoid the use of offensive language and promote respectful dialogue.\n",
        "      - Offer guidance on how to address or respond to offensive language or insults in a constructive manner.\n",
        "      <END UNSAFE CONTENT CATEGORIES>\n",
        "\n",
        "      <BEGIN CONVERSATION>\n",
        "      User: {{ user_input }}\n",
        "      <END CONVERSATION>\n",
        "\n",
        "      Provide your safety assessment for User in the above conversation:\n",
        "      - First line must read 'safe' or 'unsafe'.\n",
        "      - If unsafe, a second line must include a comma-separated list of violated categories. [/INST]\n",
        "\n",
        "\n",
        "  # Currently, the unsafe content categories are the same for both llama_guard_check_input and\n",
        "  # llama_guard_check_output prompts. Only the conversation placeholder is different.\n",
        "  # Still, we keep them separate to keep the taxonomies for input and output independently modifiable.\n",
        "  - task: llama_guard_check_output\n",
        "    content: |\n",
        "      <s>[INST] Task: Check if there is unsafe content in 'Agent' messages in conversations according our safety policy with the below categories.\n",
        "      <BEGIN UNSAFE CONTENT CATEGORIES>\n",
        "      O1: Violence and Hate.\n",
        "      Should not\n",
        "      - Help people plan or engage in violence.\n",
        "      - Encourage people to engage in violence.\n",
        "      - Express hateful, derogatory, or demeaning sentiments against people based on sensitive personal characteristics like their race, color, religion, national origin, sexual orientation, gender, gender identity, or disability.\n",
        "      - Encourage discrimination against people based on those characteristics and should not use slurs or create insults based on those characteristics.\n",
        "      Can\n",
        "      - Provide information on violence and discrimination.\n",
        "      - Discuss the topics of hate and violence, and can discuss historical events involving violence.\n",
        "      O2: Sexual Content.\n",
        "      Should not\n",
        "      - Engage in sexually explicit (i.e., erotic) conversations and should not encourage people to engage in sexual activities.\n",
        "      Can\n",
        "      - Discuss topics related to sexuality and sexual education.\n",
        "      - Provide general information regarding sexual activities.\n",
        "      - Provide guidance on topics related to sexual health.\n",
        "      O3: Criminal Planning.\n",
        "      Should not\n",
        "      - Help people plan or engage in criminal activities like theft, kidnapping, or financial crimes, especially when the person indicates that they may act on the information provided to them.\n",
        "      - Encourage people to engage in criminal activities or condone criminal behavior.\n",
        "      Can\n",
        "      - Help people understand how crimes are committed, provided that the response is not so detailed that it could be used to provide a roadmap for future crimes.\n",
        "      - Help people understand how systems work that are used to prevent crimes, provided those descriptions do not provide detailed information on how to defeat those systems.\n",
        "      O4: Guns and Illegal Weapons.\n",
        "      Should not\n",
        "      - Assist in the planning or execution of firearm or illegal weapons crimes, especially when the person indicates that they may act on the information provided to them.\n",
        "      - Encourage people to engage in firearm or illegal weapons crimes or condone such behavior.\n",
        "      Can\n",
        "      - Discuss firearms and the arguments for and against firearm ownership.\n",
        "      O5: Regulated or Controlled Substances.\n",
        "      Should not\n",
        "      - Encourage or assist people in the trafficking, consumption, or use of regulated or controlled substances, including but not limited to illegal drugs, tobacco, alcohol, and cannabis.\n",
        "      - Assist or encourage people to create such substances when it is illegal to do so.\n",
        "      Can\n",
        "      - Discuss the use of regulated or controlled substances, and can provide general information on those substances (ex: what their chemical composition is and what their effects are).\n",
        "      - Provide information on how those substances are manufactured, provided that the information is not so detailed as to provide step-by-step instructions for their creation.\n",
        "      O6: Self-Harm.\n",
        "      Should not\n",
        "      - Encourage people to harm themselves.\n",
        "      - Romanticize or condone self-harm or suicide.\n",
        "      - Provide information regarding the methods of suicide or self-harm.\n",
        "      - Provide any information or advice, other than information regarding appropriate health resources, to people who express or imply that they wish to harm themselves.\n",
        "      Can\n",
        "      - Provide general information regarding self-harm and suicide (ex: statistics, the mental health challenges faced by those who take these actions).\n",
        "      Should\n",
        "      - Provide information regarding appropriate health resources to people who imply or express the desire to harm themselves.\n",
        "      O7: Offensive Language and Insults.\n",
        "      Should not\n",
        "      - Address direct insults\n",
        "      - Use or encourage the use of swear words, bad language, or vulgar expressions.\n",
        "      - Insult, offend, or demean individuals or groups based on any personal characteristics or choices.\n",
        "      - Encourage or support the use of offensive or derogatory language.\n",
        "      Can\n",
        "      - Discuss the impact and societal perceptions of offensive language and insults.\n",
        "      - Provide information on communication strategies that avoid the use of offensive language and promote respectful dialogue.\n",
        "      - Offer guidance on how to address or respond to offensive language or insults in a constructive manner.\n",
        "      <END UNSAFE CONTENT CATEGORIES>\n",
        "\n",
        "      <BEGIN CONVERSATION>\n",
        "      User: {{ user_input }}\n",
        "\n",
        "      Agent: {{ bot_response }}\n",
        "      <END CONVERSATION>\n",
        "\n",
        "      Provide your safety assessment for Agent in the above conversation:\n",
        "      - First line must read 'safe' or 'unsafe'.\n",
        "      - If unsafe, a second line must include a comma-separated list of violated categories. [/INST]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1635f224-420c-4b07-b909-77f1b3115ef2",
      "metadata": {
        "id": "1635f224-420c-4b07-b909-77f1b3115ef2"
      },
      "source": [
        "### Testing RAG with LlamaGuard Integration\n",
        "\n",
        "#### 1. Safe Question\n",
        "We start by asking a question that is safe as per the safety-risk guidelines. As the question is asked, LlamaGuard is supposed to regard the input as '<b>safe</b>', allowing the LLM NIM to answer the question and finally respond to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b6b807c-9912-44b7-aad3-6e07b70607ad",
      "metadata": {
        "id": "5b6b807c-9912-44b7-aad3-6e07b70607ad",
        "outputId": "471a63bc-804c-4aa4-e172-970be8659659"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You can use NVIDIA AI Enterprise by first ensuring your system meets the necessary hardware and software requirements. Once you've met the requirements, you can deploy the software suite on your desired environment, such as a virtual machine or container, and then use the NVIDIA AI Enterprise console to manage and monitor your AI workloads. If you need more detailed instructions, I can provide you with a step-by-step guide or point you to our official documentation.\n"
          ]
        }
      ],
      "source": [
        "from nemoguardrails import RailsConfig, LLMRails\n",
        "config = RailsConfig.from_path(\"./config\")\n",
        "rails = LLMRails(config)\n",
        "\n",
        "response = rails.generate(messages=[{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"How do I use NVIDIA AI?\"\n",
        "}])\n",
        "print(response['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe22f322-f61e-4b02-8730-1f3440fe9ec0",
      "metadata": {
        "id": "fe22f322-f61e-4b02-8730-1f3440fe9ec0"
      },
      "source": [
        "#### 2. Unsafe Question\n",
        "\n",
        "Secondly, we can now ask a question about using NVIDIA AI Enterprise for theft planning. Since, 'theft' is a non-safe word, LlamaGuard flags the input prompt as '<b>unsafe</b>' and makes the bot refuse to respond to the user. Let us look at this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41eac75-0156-4c57-a3ea-ad896fe6b622",
      "metadata": {
        "id": "b41eac75-0156-4c57-a3ea-ad896fe6b622",
        "outputId": "8a63b068-fe2d-4346-c6ea-097c6f5f0c4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry, I can't respond to that.\n"
          ]
        }
      ],
      "source": [
        "response = rails.generate(messages=[{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Can I use NVIDIA AI Enterprise in theft planning?\"\n",
        "}])\n",
        "print(response['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b7bf8fd-9925-4c3e-b507-b6e96ca67a92",
      "metadata": {
        "id": "9b7bf8fd-9925-4c3e-b507-b6e96ca67a92"
      },
      "source": [
        "## AlignScore Integration for fact-checking the moderated LLM response\n",
        "\n",
        "A secure and safe RAG pipeline requires the LLM generated text to be factually consistent with input information and the Knowledge Base. But we need to understand what factual consistency means and how is it evaluated.\n",
        "\n",
        "<b>Facutual Consistency</b>: For a given text pair <b>(a, b)</b>, they are considered factual consistent if\n",
        "1) all the information in <b>b</b> is also present in <b>a</b>;\n",
        "2) <b>b</b> does not contradict <b>a</b>.\n",
        "\n",
        "<b>Evaluation</b>: Show the degree of factual consistency between the context (text <b>a</b>) and the claim (text <b>b</b>).\n",
        "\n",
        "Now, coming back to the RAG pipeline, the factual consistency evaluation is done by checking the LLM response with the retrieved chunks obtained from the vector database.\n",
        "\n",
        "AlignScore, a metric is developed to assess this factual consistency in context-claim pairs across various Natural Language Understanding (NLU) tasks. There are two checkpoints, 'base' and 'large' which can be integrated with NeMo Guardrails for fact-checking purposes. NeMo Guardrails toolkit allows an easy integration of AlignScore with the user's custom usecase. To do this, we first setup the [AlignScore deployment](https://docs.nvidia.com/nemo/guardrails/user_guides/advanced/align-score-deployment.html) and then integrate it into our pre-built LlamaGuard integrated guardrails configuration by adding the model endpoints into the `config.yml` file as follows\n",
        "\n",
        "### Update the LlamaGuard Integrated Guardrails configuration with AlignScore\n",
        "\n",
        "#### 1. Re-write `config.yml`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9943587-8909-4605-9668-ff1bf4ba5a62",
      "metadata": {
        "id": "a9943587-8909-4605-9668-ff1bf4ba5a62",
        "outputId": "5048f878-823c-4feb-8d18-09e897626d1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting config/config.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile config/config.yml\n",
        "instructions:\n",
        "  - type: general\n",
        "    content: |\n",
        "      Below is a conversation between a user and a bot called the NVIDIA AI Bot.\n",
        "      The bot is designed to answer questions about the NVIDIA AI Enterprise.\n",
        "      The bot is knowledgeable about the company policies.\n",
        "      If the bot does not know the answer to a question, it truthfully says it does not know.\n",
        "\n",
        "sample_conversation: |\n",
        "  user action: user said \"Hi there. Can you help me with some questions I have about NVIDIA AI Enterprise?\"\n",
        "  user intent: user express greeting and ask for assistance\n",
        "  bot intent: bot express greeting and confirm and offer assistance\n",
        "  bot action: bot say \"Hi there! I'm here to help answer any questions you may have about NVIDIA AI Enterprise. What would you like to know?\"\n",
        "  user action: user said \"What is offered by NVIDIA AI Enterprise?\"\n",
        "  user intent: user ask question about NVIDIA AI Enterprise\n",
        "  bot intent: bot respond to question about NVIDIA AI Enterprise\n",
        "  bot action: bot say \"NVIDIA AI Enterprise is a software suite that enables enterprises to easily deploy, manage, and scale AI workloads across bare-metal servers, virtual machines, and containerized environments, allowing you to accelerate your AI initiatives and optimize data center resources.\"\n",
        "\n",
        "models:\n",
        "    - type: main\n",
        "      engine: nim\n",
        "      model: meta/llama-3.1-70b-instruct\n",
        "\n",
        "    - type: llama_guard\n",
        "      engine: vllm_openai\n",
        "      parameters:\n",
        "          openai_api_base: \"http://localhost:5123/v1\"\n",
        "          model_name: \"meta-llama/LlamaGuard-7b\"\n",
        "\n",
        "rails:\n",
        "    config:\n",
        "      fact_checking:\n",
        "        provider: align_score\n",
        "        parameters:\n",
        "          endpoint: \"http://localhost:5000/alignscore_base\"\n",
        "    output:\n",
        "          flows:\n",
        "          - alignscore check facts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a318095e-29ad-4e3a-8017-f6aa613f6df3",
      "metadata": {
        "id": "a318095e-29ad-4e3a-8017-f6aa613f6df3"
      },
      "source": [
        "#### 2. Adding flows for Fact-Checking using the AlignScore model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57548067-8fac-46df-b9b8-21d4e13267eb",
      "metadata": {
        "id": "57548067-8fac-46df-b9b8-21d4e13267eb",
        "outputId": "d8a06593-43cd-4b54-bb29-1c679eb1fd4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing config/factchecking.co\n"
          ]
        }
      ],
      "source": [
        "%%writefile config/factchecking.co\n",
        "define flow\n",
        "  user ask about report\n",
        "  $check_facts = True\n",
        "  bot provide report answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e2b03d6-6c80-4dd7-a020-3c72b8463bf7",
      "metadata": {
        "id": "2e2b03d6-6c80-4dd7-a020-3c72b8463bf7"
      },
      "source": [
        "### Testing the AlignScore Integration with the prebuilt LlamaGuard Integrated guardrails configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f097460f-09e8-42e3-86dd-f52e0e0b2993",
      "metadata": {
        "id": "f097460f-09e8-42e3-86dd-f52e0e0b2993",
        "outputId": "17dcc492-d2af-49fe-8dd3-79be0b9d5d1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To install NVIDIA AI Enterprise software components using Kubernetes, you can use Helm charts. First, add the NVIDIA Helm repository and update your local repository cache. Then, install the necessary charts, such as the NVIDIA AI Enterprise suite or individual components like RAPIDS or TensorRT, using the helm install command. You can find more detailed instructions and specific chart names in the NVIDIA AI Enterprise documentation.\n"
          ]
        }
      ],
      "source": [
        "config = RailsConfig.from_path(\"./config\")\n",
        "rails = LLMRails(config)\n",
        "\n",
        "response = rails.generate(messages=[{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"How do I install NVIDIA AI Enterprise Software Components using Kubernetes?\"\n",
        "}])\n",
        "print(response['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0035dd84-961d-4a61-bffd-22369e590ded",
      "metadata": {
        "id": "0035dd84-961d-4a61-bffd-22369e590ded"
      },
      "source": [
        "As we can see from the response, the consistency of the information is evaluated using AlignScore and maintained with that in the Knowledge base"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
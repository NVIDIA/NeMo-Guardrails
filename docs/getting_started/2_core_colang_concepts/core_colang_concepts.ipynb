{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Core Colang Concepts\n",
    "\n",
    "This guide builds on the previous [Hello World guide](../1_hello_world/README.md) and introduces the core Colang concepts you should understand to get started with NeMo Guardrails."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Init: copy the previous config.\n",
    "!cp -r ../1_hello_world/config ."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is Colang?\n",
    "\n",
    "Colang is a modeling language for conversational applications. Using Colang you can design how the conversation between a user and a **bot** should happen.\n",
    "\n",
    "> **NOTE**: throughout this guide, the term *bot* is used to mean the entire LLM-based Conversational Application.\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "In Colang, the two core concepts are: **messages** and **flows**.\n",
    "\n",
    "### Messages\n",
    "\n",
    "In Colang, a conversation is modeled as an exchange of messages between a user and a bot. An exchanged **message** has an **utterance**, e.g. *\"What can you do?\"*, and a **canonical form**, e.g. `ask about capabilities`. A canonical form is a paraphrase of the utterance to a standard, usually shorter, form.\n",
    "\n",
    "Using Colang, you can define the user messages that are important for your LLM-based application. For example, in the \"Hello World\" example, the `express greeting` user message is defined as:\n",
    "\n",
    "```\n",
    "define user express greeting\n",
    "  \"Hello\"\n",
    "  \"Hi\"\n",
    "  \"Wassup?\"\n",
    "```\n",
    "\n",
    "The `express greeting` represents the canonical form and \"Hello\", \"Hi\" and \"Wassup?\" represent example utterances. The role of the example utterances is to teach the bot the meaning of a defined canonical form.\n",
    "\n",
    "You can also define bot messages, i.e. how the bot should talk to the user. For example, in the \"Hello World\" example, the `express greeting` and `ask how are you` bot messages are defined as:\n",
    "\n",
    "```\n",
    "define bot express greeting\n",
    "  \"Hey there!\"\n",
    "\n",
    "define bot ask how are you\n",
    "  \"How are you doing?\"\n",
    "```\n",
    "\n",
    "If more than one utterance are given for a canonical form, a random one will be used whenever the message is used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Are the *user message canonical forms* the same thing as classical intents?**\n",
    "\n",
    "Yes, you can think of them as intents. However, when using them, the bot is not constrained to use only the pre-defined list."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Flows\n",
    "\n",
    "In Colang, **flows** represent patterns of interaction between the user and the bot. In their simplest form, they are sequences of user and bot messages. In the \"Hello World\" example, the `greeting` flow is defined as:\n",
    "\n",
    "```colang\n",
    "define flow greeting\n",
    "  user express greeting\n",
    "  bot express greeting\n",
    "  bot ask how are you\n",
    "```\n",
    "\n",
    "Intuitively, this flow instructs the bot to respond with a greeting and ask how the user is feeling every time the user greets the bot."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Guardrails\n",
    "\n",
    "Messages and flows provide the core building blocks for defining **guardrails** (or \"rails\" for short). The `greeting` flow above is in fact a **rail** that guides the LLM how to respond to a greeting.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How does it work?\n",
    "\n",
    "Before moving further, let's take a closer look at what happens under the hood. Some of the questions that we are going to answer are:\n",
    "\n",
    "- How are the user and bot message definitions used?\n",
    "- How exactly is the LLM prompted and how many calls are made?\n",
    "- Can I use bot messages without example utterances?\n",
    "\n",
    "Let's run again the greeting example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there!\n",
      "How are you doing?\n"
     ]
    }
   ],
   "source": [
    "from nemoguardrails import RailsConfig, LLMRails\n",
    "\n",
    "config = RailsConfig.from_path(\"./config\")\n",
    "rails = LLMRails(config)\n",
    "\n",
    "response = await rails.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hello!\"\n",
    "}])\n",
    "print(response[\"content\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The `explain` feature\n",
    "\n",
    "To get more visibility on what happens under the hood, we will make use of the *explain* feature that the `LLMRails` class provides."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# We fetch the latest `ExplainInfo` object using the `explain` method.\n",
    "info = rails.explain()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Colang History\n",
    "\n",
    "Firstly, we can check the history of the conversation in Colang format. This shows us the exact messages and their canonical forms:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user \"Hello!\"\n",
      "  express greeting\n",
      "bot express greeting\n",
      "  \"Hey there!\"\n",
      "bot ask how are you\n",
      "  \"How are you doing?\"\n"
     ]
    }
   ],
   "source": [
    "print(info.colang_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### LLM Calls\n",
    "\n",
    "Secondly, we can check the actual LLM calls that have been made:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: 1 LLM call(s) took 1.42 seconds and used 564 tokens.\n",
      "\n",
      "1. Task `generate_user_intent` took 1.42 seconds and used 564 tokens.\n"
     ]
    }
   ],
   "source": [
    "info.print_llm_calls_summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `info` object also contains an `info.llm_calls` attribute with detailed information about each LLm call. We will look at this shortly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The process\n",
    "\n",
    "Once an input message is received from the user, a multi-step process begins.\n",
    "\n",
    "### Step 1: compute user message canonical form\n",
    "\n",
    "After an utterance is received from the user (e.g., \"Hello!\" in the example above), the guardrails instance will compute the corresponding canonical form. By default, the LLM itself is used to perform this task.\n",
    "\n",
    "> **NOTE**: NeMo Guardrails uses a task-oriented interaction model with the LLM. Every time the LLM is called, a specific task prompt template is used, e.g. `generate_user_intent`, `generate_next_step`, `generate_bot_message`. The default template prompts can be found [here](../../../nemoguardrails/llm/prompts/general.yml).\n",
    "\n",
    "In the case of the \"Hello!\" message, a single LLM call was made using the `generate_user_intent` task prompt template. Let's see how the prompt looks like:\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Below is a conversation between a helpful AI assistant and a user. The bot is designed to generate human-like text based on the input that it receives. The bot is talkative and provides lots of specific details. If the bot does not know the answer to a question, it truthfully says it does not know.\n",
      "\"\"\"\n",
      "\n",
      "# This is how a conversation between a user and the bot can go:\n",
      "user \"Hello there!\"\n",
      "  express greeting\n",
      "bot express greeting\n",
      "  \"Hello! How can I assist you today?\"\n",
      "user \"What can you do for me?\"\n",
      "  ask about capabilities\n",
      "bot respond about capabilities\n",
      "  \"As an AI assistant, I can help you with a wide range of tasks. This includes question answering on various topics, generating text for various purposes and providing suggestions based on your preferences.\"\n",
      "user \"Tell me a bit about the history of NVIDIA.\"\n",
      "  ask general question\n",
      "bot response for general question\n",
      "  \"NVIDIA is a technology company that specializes in designing and manufacturing graphics processing units (GPUs) and other computer hardware. The company was founded in 1993 by Jen-Hsun Huang, Chris Malachowsky, and Curtis Priem.\"\n",
      "user \"tell me more\"\n",
      "  request more information\n",
      "bot provide more information\n",
      "  \"Initially, the company focused on developing 3D graphics processing technology for the PC gaming market. In 1999, NVIDIA released the GeForce 256, the world's first GPU, which was a major breakthrough for the gaming industry. The company continued to innovate in the GPU space, releasing new products and expanding into other markets such as professional graphics, mobile devices, and artificial intelligence.\"\n",
      "user \"thanks\"\n",
      "  express appreciation\n",
      "bot express appreciation and offer additional help\n",
      "  \"You're welcome. If you have any more questions or if there's anything else I can help you with, please don't hesitate to ask.\"\n",
      "\n",
      "\n",
      "# This is how the user talks:\n",
      "user \"Wassup?\"\n",
      "  express greeting\n",
      "\n",
      "user \"Hi\"\n",
      "  express greeting\n",
      "\n",
      "user \"Hello\"\n",
      "  express greeting\n",
      "\n",
      "\n",
      "\n",
      "# This is the current conversation between the user and the bot:\n",
      "# Choose intent from this list: express greeting\n",
      "user \"Hello there!\"\n",
      "  express greeting\n",
      "bot express greeting\n",
      "  \"Hello! How can I assist you today?\"\n",
      "user \"What can you do for me?\"\n",
      "  ask about capabilities\n",
      "bot respond about capabilities\n",
      "  \"As an AI assistant, I can help you with a wide range of tasks. This includes question answering on various topics, generating text for various purposes and providing suggestions based on your preferences.\"\n",
      "user \"Hello!\"\n"
     ]
    }
   ],
   "source": [
    "print(info.llm_calls[0].prompt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The prompt has four logical sections:\n",
    "\n",
    "1. A set of general instructions. These can [be configured](../../user_guides/configuration-guide.md#general-instructions) using the `instructions` key in `config.yml`.\n",
    "\n",
    "2. A sample conversation, which can also [be configured](../../user_guides/configuration-guide.md#sample-conversation) using the `sample_conversation` key in `config.yml`.\n",
    "\n",
    "3. A set of examples for converting user utterances to canonical forms. The top five most relevant examples are chosen by performing a vector search against all the examples. For more details check out [TODO](#).\n",
    "\n",
    "4. The current conversation, pre-seeded with the first two turns from the sample conversation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the `generate_user_intent` task, the LLM must predict the canonical form for the last user utterance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  express greeting\n",
      "bot express greeting\n",
      "  \"Hello! How can I assist you today?\"\n"
     ]
    }
   ],
   "source": [
    "print(info.llm_calls[0].completion)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see, the LLM correctly predicted the `express greeting` canonical form. It even went further to predict what the bot should do, i.e. `bot express greeting`, and the utterance that should be used. However, for the `generate_user_intent` task, only the first predicted line is used. If you want the LLM to predict everything in a single call, you can enable the [`rails.dialog.single_call` option](#) in `config.yml`.\n",
    "\n",
    "### Step 2: decide next step\n",
    "\n",
    "After the canonical form for the user message has been determined, the guardrails instance needs to decide what should happen next. There are two cases:\n",
    "\n",
    "1. If there is a flow that matches the canonical form, then it will be used. The flow can decide that the bot should respond with a certain message, or execute an action.\n",
    "2. If there is no flow, the LLM is prompted for the next step, i.e. the `generate_next_step` task.\n",
    "\n",
    "In our example, there was a match from the `greeting` flow and the next steps are:\n",
    "\n",
    "```\n",
    "bot express greeting\n",
    "bot ask how are you\n",
    "```\n",
    "\n",
    "### Step 3: generate bot message\n",
    "\n",
    "Once the canonical form for what the bot should say has been decided, the actual message needs to be generated. And here we have two cases as well:\n",
    "\n",
    "1. If a predefined message is found, the exact utterance is used. If more than one example utterances are associated with the same canonical form, a random one will be used.\n",
    "2. If a predefined message does not exist, the LLM will be prompted to generate the message, i.e. the `generate_bot_message` \n",
    "\n",
    "In our \"Hello World\" example, the predefined messages \"Hello world!\" and \"How are you doing?\" have been used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The followup question\n",
    "\n",
    "In the above example, we've seen a case where the LLM was prompted only once. The figure below provides a summary of the outlined sequence of steps:\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"../../_assets/puml/core_colang_concepts_fig_1.png\" style=\"max-width: 486px;\">\n",
    "</p>\n",
    "\n",
    "\n",
    "Now, let's look at the same process described above, on the followup question \"What is the capital of France?\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris. It is located in the northern region of the country and is the most populous city in the country.\n"
     ]
    }
   ],
   "source": [
    "response = await rails.generate_async(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What is the capital of France?\"\n",
    "}])\n",
    "print(response[\"content\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check the colang history:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user \"What is the capital of France?\"\n",
      "  ask general question\n",
      "bot general response\n",
      "  \"The capital of France is Paris. It is located in the northern region of the country and is the most populous city in the country.\"\n"
     ]
    }
   ],
   "source": [
    "info = rails.explain()\n",
    "print(info.colang_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And the LLM calls:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: 3 LLM call(s) took 4.81 seconds and used 1451 tokens.\n",
      "\n",
      "1. Task `generate_user_intent` took 1.58 seconds and used 570 tokens.\n",
      "2. Task `generate_next_steps` took 1.66 seconds and used 228 tokens.\n",
      "3. Task `generate_bot_message` took 1.56 seconds and used 653 tokens.\n"
     ]
    }
   ],
   "source": [
    "info.print_llm_calls_summary()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the above we can see that the `ask general question` canonical form is predicted for the user utterance \"What is the capital of France?\". Because there is no flow that matches it, the LLM is asked to predict the next step, which in this case is `bot general response`. And because there is no predefined response, the LLM is asked a third time to predict the final message.\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"../../_assets/puml/core_colang_concepts_fig_2.png\" style=\"max-width: 686px;\">\n",
    "</p>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrapping up\n",
    "\n",
    "This guide has provided a detailed overview of two core Colang concepts: *messages* and *flows*. We've also looked at how the message and flow definitions are used under the hood and how the LLM is prompted.\n",
    "\n",
    "## Next\n",
    "\n",
    "In the next guides, we will look at how these core process is used to implement different types of rails (input, output, dialog, etc.)."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
